{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\">Imports</a></span><ul class=\"toc-item\"><li><span><a href=\"#Specify-Tesseract-install-location\" data-toc-modified-id=\"Specify-Tesseract-install-location-1.1\">Specify Tesseract install location</a></span></li></ul></li><li><span><a href=\"#Choose-product-category\" data-toc-modified-id=\"Choose-product-category-2\">Choose product category</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-3\">Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-data-for-all-products\" data-toc-modified-id=\"Download-data-for-all-products-3.1\">Download data for all products</a></span></li><li><span><a href=\"#Download-and-clean-data-for-a-single-product\" data-toc-modified-id=\"Download-and-clean-data-for-a-single-product-3.2\">Download and clean data for a single product</a></span></li><li><span><a href=\"#Scrape-price-data-for-a-single-product\" data-toc-modified-id=\"Scrape-price-data-for-a-single-product-3.3\">Scrape price data for a single product</a></span></li><li><span><a href=\"#Find-the-closest-date-in-a-list\" data-toc-modified-id=\"Find-the-closest-date-in-a-list-3.4\">Find the closest date in a list</a></span></li><li><span><a href=\"#Locate-a-line-in-an-image\" data-toc-modified-id=\"Locate-a-line-in-an-image-3.5\">Locate a line in an image</a></span></li><li><span><a href=\"#Impute-NaN-values-in-price-data\" data-toc-modified-id=\"Impute-NaN-values-in-price-data-3.6\">Impute NaN values in price data</a></span></li></ul></li><li><span><a href=\"#Download-the-dataset\" data-toc-modified-id=\"Download-the-dataset-4\">Download the dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't actually need to run this notebook unless you plan to make your own dataset (e.g., for a different product category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:52:18.055803Z",
     "start_time": "2020-06-27T04:52:18.049790Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import skimage\n",
    "from skimage import io\n",
    "import difflib\n",
    "import pytesseract\n",
    "import datetime\n",
    "import re\n",
    "import os.path\n",
    "from bisect import bisect_left\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Tesseract install location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T05:10:00.513205Z",
     "start_time": "2020-06-27T05:10:00.509243Z"
    }
   },
   "source": [
    "You will need to install\n",
    "[Tesseract OCR](https://www.pyimagesearch.com/2017/07/10/using-tesseract-ocr-python/)\n",
    "before you can use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:51:21.469347Z",
     "start_time": "2020-06-27T04:51:21.465356Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here, we specify the Tesseract OCR installation location.\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'D:\\Software\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose product category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compiled two datasets: books and board games. \n",
    "You can easily create your own list of items in a similar format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T05:12:25.781825Z",
     "start_time": "2020-06-27T05:12:25.458688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the dataset to download, and the download location.\n",
    "product = 'book_db.csv'\n",
    "output_folder = 'paperbacks'\n",
    "#product = 'game_db.csv'\n",
    "#output_folder = 'boardgames'\n",
    "\n",
    "os.mkdir(os.path.join('data', output_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data for all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:53:27.324456Z",
     "start_time": "2020-06-27T04:53:27.320467Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data(product_ids, output_folder):\n",
    "    for product_id in product_ids:\n",
    "        df = collect_data(product_id)\n",
    "        if df is not None:\n",
    "            df.to_csv(os.path.join('data', output_folder, product_id + '.csv'),\n",
    "                      index=False)\n",
    "        else:\n",
    "            print('id ' + product_id + ' failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and clean data for a single product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:52:49.830963Z",
     "start_time": "2020-06-27T04:52:49.825016Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_data(asin):\n",
    "    \"\"\" \n",
    "    Get the price history of an item from Amazon.\n",
    "  \n",
    "    Parameters: \n",
    "    asin (str): Amazon Standard Identification Number for an item.\n",
    "  \n",
    "    Returns: \n",
    "    features: numpy array of features of the price history that can \n",
    "        be used as inputs to a trained random forest classifier\n",
    "    history: A pandas dataframe of the item's price at 12-hours\n",
    "        time resolution, collected from camelcamelcamel.com\n",
    "    \"\"\"\n",
    "    # Set the size of the image to be downloaded. Larger sizes\n",
    "    # will product more accurate results at the cost of\n",
    "    # increased processing time.\n",
    "    image_width = 6555\n",
    "    image_height = 1013\n",
    "\n",
    "    # get the URL to the camelcamelcamel page\n",
    "    url = 'https://charts.camelcamelcamel.com/us/' + asin + \\\n",
    "    '/amazon.png?force=0&zero=1&w='+str(image_width)+'&h='+str(image_height)+\\\n",
    "    '&desired=false&legend=0&ilt=1&tp=all&fo=0&lang=en'\n",
    "\n",
    "    # Extract price data for times when the item was sold by Amazon\n",
    "    dates, prices = scrape_prices(url, image_width, image_height)\n",
    "\n",
    "    # If the price data are insufficient, notify the user\n",
    "    if dates is None or \\\n",
    "       np.size(dates) < 60 or \\\n",
    "       np.sum(np.isnan(prices))/np.size(dates) > .3:\n",
    "        return None, None\n",
    "\n",
    "    # Impute nans\n",
    "    prices = impute_nan(prices)\n",
    "    # Convert the price history to a dataframe\n",
    "    history = pd.DataFrame({'date': dates, 'price': prices})\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape price data for a single product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:51:25.835137Z",
     "start_time": "2020-06-27T04:51:25.800207Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_prices(url, image_width, image_height):\n",
    "    \"\"\" \n",
    "    Extract dates and prices from a camelcamelcamel item URL.\n",
    "  \n",
    "    Parameters: \n",
    "    url (str): camelcamelcamel URL for a product.\n",
    "    image_width (int): width of the image to be used, in pixels\n",
    "    image_height (int): height of the image to be used, in pixels\n",
    "  \n",
    "    Returns: \n",
    "    dates: numpy array of dates at 12-hour intervals\n",
    "    prices: a numpy array of prices\n",
    "    \"\"\"\n",
    "\n",
    "    ################\n",
    "    # Collect data #\n",
    "    ################\n",
    "\n",
    "    # Define colors of elements of the plot (RGB)\n",
    "    # Plotted lines\n",
    "    plot_colors = np.array([[194, 68, 68], [119, 195, 107], [51, 51, 102]])\n",
    "    # Gray axis lines\n",
    "    gray = np.array([215, 215, 214])\n",
    "    # Black axis lines\n",
    "    black = np.array([75, 75, 75])\n",
    "\n",
    "    # Download the image\n",
    "    response = requests.get(url)\n",
    "    image_temp = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Convert image to float\n",
    "    im = np.array(image_temp)\n",
    "\n",
    "    # Get masks for each plot color\n",
    "    masks = list()\n",
    "    for i in range(3):\n",
    "        masks.append(np.all(im == plot_colors[i], axis=-1))\n",
    "\n",
    "    # Check if there image is empty (camel has no data)\n",
    "    if not np.any(masks[1]):\n",
    "        return None, None\n",
    "\n",
    "    ######################\n",
    "    # Find x and y scale #\n",
    "    ######################\n",
    "\n",
    "    # Find the y axis upper limit\n",
    "    # Crop a portion of the image containing the top of the grid\n",
    "    top_line_crop = im[:, round(image_width * .5) - 5:round(image_width * .5) + 6, :]\n",
    "    # Get the position of the line\n",
    "    line_y_value = find_line(top_line_crop, gray)\n",
    "\n",
    "    # If it wasn't found, quit\n",
    "    # Checks of this nature are rarely needed, as long\n",
    "    # as camel keeps their plotting code the same\n",
    "    if line_y_value is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        line_y_value = int(line_y_value)\n",
    "\n",
    "    # Find x axis limits\n",
    "    # Crop the left-most and right-most vertical lines in the grid\n",
    "    left_line_crop = np.transpose(\n",
    "        im[round(image_height * .5) - 8:round(image_height * .5) +\n",
    "           9, :round(image_width * .1), :],\n",
    "        axes=[1, 0, 2])\n",
    "    right_line_crop = np.transpose(im[round(image_height * .5) -\n",
    "                                      8:round(image_height * .5) + 9,\n",
    "                                      round(image_width * .7):, :],\n",
    "                                   axes=[1, 0, 2])\n",
    "    lo_x_value = find_line(left_line_crop, black)\n",
    "    hi_x_value = find_line(right_line_crop[::-1, :, :], gray)\n",
    "    if lo_x_value is None or hi_x_value is None:\n",
    "        return None, None\n",
    "    else:\n",
    "        lo_x_value = int(lo_x_value)\n",
    "        hi_x_value = int(hi_x_value)\n",
    "\n",
    "    # Find price corresponding to the y axis upper limit\n",
    "    # First, crop the price text\n",
    "    upper_price_crop = im[line_y_value - 8:line_y_value + 10,\n",
    "                          0:lo_x_value - 9, :]\n",
    "    upper_price_crop = Image.fromarray(upper_price_crop)\n",
    "    # Resize and apply OCR\n",
    "    upper_price_crop = upper_price_crop.resize(\n",
    "        (upper_price_crop.width * 12, upper_price_crop.height * 12))\n",
    "    upper_price_string = pytesseract.image_to_string(upper_price_crop)\n",
    "    upper_price = float(upper_price_string[1:].replace(',', ''))\n",
    "\n",
    "    # Store y position of price limits\n",
    "    # The position and price of the lower limit are constant\n",
    "    limit_y_positions = np.array([line_y_value, image_height - 49])\n",
    "\n",
    "    # Calculate dollars per pixel\n",
    "    dollarspp = upper_price / (np.max(limit_y_positions) -\n",
    "                               np.min(limit_y_positions))\n",
    "\n",
    "    # Crop year text from bottom of image so that we\n",
    "    # can find the date of the first timepoint\n",
    "    year_crop = im[-14:, 0:round(image_width / 8), :]\n",
    "    year_crop = Image.fromarray(year_crop)\n",
    "    # Resize and apply OCR\n",
    "    year_crop = year_crop.resize((year_crop.width * 5, year_crop.height * 5))\n",
    "    year_string = pytesseract.image_to_string(year_crop, config='--psm 7')\n",
    "    year_string = year_string[:4]\n",
    "\n",
    "    # Crop month and day from bottom left corner\n",
    "    date_crop = im[-49:-14, (lo_x_value - 40):(lo_x_value + 6), :]\n",
    "    # Convert to image\n",
    "    date_crop = Image.fromarray(date_crop)\n",
    "    # Invert, so that rotation works\n",
    "    date_crop = ImageOps.invert(date_crop)\n",
    "    # Pad the image\n",
    "    date_crop_padded = Image.new(\n",
    "        'RGB', (round(date_crop.width * 1.5), round(date_crop.height * 1.5)),\n",
    "        (0, 0, 0))\n",
    "    date_crop_padded.paste(date_crop, box=(0, round(date_crop.height * .5)))\n",
    "    # Resize\n",
    "    date_crop_padded = date_crop_padded.resize(\n",
    "        (date_crop_padded.width * 7, date_crop_padded.height * 7),\n",
    "        resample=Image.LANCZOS)\n",
    "    # Rotate and invert\n",
    "    date_crop_padded = ImageOps.invert(date_crop_padded.rotate(-45))\n",
    "    # Crop\n",
    "    date_crop_padded = date_crop_padded.crop((1,85,297,260))\n",
    "    # Apply OCR\n",
    "    date_string = pytesseract.image_to_string(date_crop_padded)\n",
    "    # Find closest match to a month\n",
    "    start_month = difflib.get_close_matches(date_string, [\n",
    "        'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct',\n",
    "        'Nov', 'Dec'\n",
    "    ],\n",
    "                                            n=1,\n",
    "                                            cutoff=0.2)\n",
    "    # Quit if no month was found\n",
    "    if np.size(start_month) < 1:\n",
    "        return None, None\n",
    "\n",
    "    start_month = start_month[0]\n",
    "\n",
    "    # Get the day of the first timepoint\n",
    "    # Try to fix mixups between 'o' and 0\n",
    "    if date_string[-1] == 'o':\n",
    "        date_string = date_string[:-1] + '0'\n",
    "    # Remove all whitespace\n",
    "    date_string_stripped = \"\".join(date_string.split())\n",
    "    # Take last 2 digits if the second-to-last is reasonable\n",
    "    if date_string_stripped[-2].isdigit() and 0 < int(\n",
    "            date_string_stripped[-2]) < 4:\n",
    "        start_day = date_string_stripped[-2:]\n",
    "    else:\n",
    "        start_day = '0' + date_string_stripped[-1]\n",
    "\n",
    "    # Store x axis locations of time limits\n",
    "    limit_x_positions = [lo_x_value, image_width - hi_x_value]\n",
    "\n",
    "    # Check if our date is valid\n",
    "    try:\n",
    "        start_time = datetime.datetime.strptime(\n",
    "            start_month + start_day + year_string, '%b%d%Y')\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "    # Get current time\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    # Calculate days per pixel\n",
    "    time_delta = end_time - start_time\n",
    "    dayspp = time_delta.days / int(1 + np.diff(limit_x_positions))\n",
    "\n",
    "    # Get number of observations\n",
    "    num_obs = int(np.diff(limit_x_positions))\n",
    "\n",
    "    # Preallocate prices as nan\n",
    "    prices = np.ones(num_obs) * np.nan\n",
    "\n",
    "    ##################\n",
    "    # Extract prices #\n",
    "    ##################\n",
    "\n",
    "    # Find y-axis value of blue pixels in each time step -\n",
    "    # these are the prices we're looking for\n",
    "    y = [[i for i, x in enumerate(q) if x] for q in np.transpose(\n",
    "        masks[2][:, limit_x_positions[0]:limit_x_positions[1]])]\n",
    "\n",
    "    # Adjust values if necessary, then convert to prices\n",
    "    # Missing data are set to nan\n",
    "    for i in range(num_obs):\n",
    "        # Check if the bottom of the blue line is covered by a red or green line\n",
    "        if np.size(y[i]) == 1:\n",
    "            if masks[0][int(y[i][0]) + 1, limit_x_positions[0] +\n",
    "                        i] or masks[1][int(y[i][0]) + 1,\n",
    "                                       limit_x_positions[0] + i, ]:\n",
    "                y[i][0] += 1\n",
    "\n",
    "        # Check if the blue line is covered by both red and green lines\n",
    "        if np.size(y[i]) == 0:\n",
    "            red_idx = [q for q, x in enumerate(masks[0][:, limit_x_positions[0] + i]) if x]\n",
    "            grn_idx = [q for q, x in enumerate(masks[1][:, limit_x_positions[0] + i]) if x]\n",
    "            if np.size(red_idx) == 1 and np.size(grn_idx) == 1 and np.abs(\n",
    "                    int(red_idx[0]) - int(grn_idx[0])) == 1:\n",
    "                y[i] = grn_idx\n",
    "            else:\n",
    "                y[i] = np.nan\n",
    "\n",
    "        prices[i] = dollarspp * (image_height - np.max(y[i]) - 50)\n",
    "\n",
    "    # Adjust periods with no data\n",
    "    # First, find nans and convert to a str for regex searching\n",
    "    nans = ''.join([str(int(np.isnan(i))) for i in prices])\n",
    "    # Ensure the beginnings of empty periods are correct\n",
    "    matches = [m.span() for m in re.finditer('000110011', nans)]\n",
    "    for match in matches:\n",
    "        prices[match[0] + 3:match[0] + 5] = prices[match[0] + 5]\n",
    "    # Then remove empty periods\n",
    "    nans = ''.join([str(int(np.isnan(i))) for i in prices])\n",
    "    matches = [m.span() for m in re.finditer('1100', nans)]\n",
    "    for match in matches:\n",
    "        prices[match[0] + 2:match[0] + 4] = np.nan\n",
    "\n",
    "    ###################\n",
    "    # Resample prices #\n",
    "    ###################\n",
    "\n",
    "    # Resample to 2x daily observations at 6:00 and 18:00\n",
    "    # First, get the dates of our observations\n",
    "    dates = pd.date_range(start_time, end_time,\n",
    "                          periods=num_obs).to_pydatetime()\n",
    "    # Initialize new dates and prices at the desired interval\n",
    "    dates_2x_daily = pd.date_range(datetime.datetime(start_time.year,\n",
    "                                                     start_time.month,\n",
    "                                                     start_time.day, 6),\n",
    "                                   datetime.datetime(end_time.year,\n",
    "                                                     end_time.month,\n",
    "                                                     end_time.day, 18),\n",
    "                                   freq='12H').to_pydatetime()\n",
    "    prices_2x_daily = np.ones(np.size(dates_2x_daily)) * np.nan\n",
    "\n",
    "    # Find price at the closest date to each timepoint\n",
    "    for i in range(np.size(dates_2x_daily)):\n",
    "        prices_2x_daily[i] = prices[take_closest_date(dates -\n",
    "                                                      dates_2x_daily[i])]\n",
    "\n",
    "    # Make sure most recent price is correct\n",
    "    prices_2x_daily[-1] = prices[-1]\n",
    "    # Round prices to 2 decimal places\n",
    "    prices_2x_daily = np.around(prices_2x_daily, 2)\n",
    "\n",
    "    return dates_2x_daily, prices_2x_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the closest date in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:51:28.709037Z",
     "start_time": "2020-06-27T04:51:28.704050Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function is a modified version of the one posted by\n",
    "# Lauritz V. Thaulow on stackoverflow at\n",
    "# https://stackoverflow.com/a/12141511\n",
    "def take_closest_date(myList):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns index of closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the index of the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, datetime.timedelta(0))\n",
    "    if pos == 0:\n",
    "        return 0\n",
    "    if pos == len(myList):\n",
    "        return len(myList) - 1\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if abs(after) < abs(before):\n",
    "        return pos\n",
    "    else:\n",
    "        return pos - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate a line in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:51:29.917964Z",
     "start_time": "2020-06-27T04:51:29.912011Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_line(img, c):\n",
    "    \"\"\" \n",
    "    Find the position of a line that bisects an image horizontally. \n",
    "    If there are multiple lines, only the first is returned.\n",
    "  \n",
    "    Parameters: \n",
    "    img (float): numpy array containing the image to search.\n",
    "    c (float): 3-element numpy array equal to the line's color.\n",
    "  \n",
    "    Returns: \n",
    "    int: Location of the line, in pixels from the top of the image.\n",
    "  \n",
    "    \"\"\"\n",
    "    # Colors in the image should be withing this range of the target\n",
    "    color_tolerance = 15\n",
    "    # This fraction of the image should contain matching colors\n",
    "    # to be considered a line\n",
    "    match_threshold = .75\n",
    "    # Get the width of the image, in pixels\n",
    "    img_width = np.size(img, 1)\n",
    "    # Find all pixels within color tolerance\n",
    "    img[(img < c - color_tolerance) | (img > c + color_tolerance)] = 0\n",
    "    img[img > 0] = 1\n",
    "    # Only take pixels where all channels are within color tolerance\n",
    "    mask2d = np.all(img, axis=-1)\n",
    "    # Sum across columns\n",
    "    sums = np.sum(mask2d, axis=1)\n",
    "    # Find rows with sufficient matches\n",
    "    matches = np.argwhere(sums > match_threshold * img_width)\n",
    "    if np.size(matches) < 1:\n",
    "        return\n",
    "    else:\n",
    "        return matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN values in price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:51:31.149508Z",
     "start_time": "2020-06-27T04:51:31.144522Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_nan(x):\n",
    "    \"\"\" \n",
    "    Impute nan values in a series of prices using most recent non-nan values.\n",
    "  \n",
    "    Parameters: \n",
    "    x (numpy array): A series of prices.\n",
    "  \n",
    "    Returns: \n",
    "    Array with nan values imputed.\n",
    "    \"\"\"\n",
    "    # Convert result of isnan to a string for regex searching\n",
    "    nans = ''.join([str(int(np.isnan(i))) for i in x])\n",
    "    # Find groups of nans\n",
    "    matches = [m.span() for m in re.finditer('0[^0]+', nans)]\n",
    "    # Replace each with most recent non-nan value\n",
    "    for match in matches:\n",
    "        x[match[0] + 1:match[1]] = x[match[0]]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T05:06:32.735893Z",
     "start_time": "2020-06-27T05:06:32.732899Z"
    }
   },
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to actually download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T04:55:18.559676Z",
     "start_time": "2020-06-27T04:54:54.352774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the list of item URLs\n",
    "db = pd.read_csv(os.path.join('data', 'links', product))\n",
    "# Extract the ASIN from each\n",
    "product_ids = db['Camel link'].str.slice(start=-10)\n",
    "\n",
    "download_data(product_ids, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
